{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "File \u001b[1;32mc:\\Users\\Daffa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[0;32m     34\u001b[0m     )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://id.jobstreet.com/Sistem-Informasi-jobs'\n",
    "url = 'https://id.jobstreet.com/api/jobsearch/v5/me/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=279354b3755c5f7b91ccc1391bee4d41-6421044&userid=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&usersessionid=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&eventCaptureSessionId=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&page=1&keywords=Information+Systems&pageSize=32&include=seodata,relatedsearches,joracrosslink,gptTargeting,pills&baseKeywords=Information+Systems&locale=id-ID&seekerId=581997302&solId=c64d160c-5b72-476a-bc37-3cf7a83a85fc&relatedSearchesCount=12'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.98',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlBQcjNlT0dQeFAwa2FRcnFxandmTyJ9.eyJodHRwOi8vc2Vlay9jbGFpbXMvY291bnRyeSI6IklEIiwiaHR0cDovL3NlZWsvY2xhaW1zL2JyYW5kIjoiam9ic3RyZWV0IiwiaHR0cDovL3NlZWsvY2xhaW1zL2V4cGVyaWVuY2UiOiJjYW5kaWRhdGUiLCJodHRwOi8vc2Vlay9jbGFpbXMvdXNlcl9pZCI6IjU4MTk5NzMwMiIsImlzcyI6Imh0dHBzOi8vbG9naW4uc2Vlay5jb20vIiwic3ViIjoiYXV0aDB8NjcxZWQ1MDVlODFiZjkwMDc0MDc5NDVlIiwiYXVkIjpbImh0dHBzOi8vc2Vlay9hcGkvY2FuZGlkYXRlIiwiaHR0cHM6Ly9zZWVrYW56Lm9ubGluZWF1dGgucHJvZC5vdXRmcmEueHl6L3VzZXJpbmZvIl0sImlhdCI6MTc0MjgwNjQyMSwiZXhwIjoxNzQyODEwMDIxLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIG9mZmxpbmVfYWNjZXNzIiwiYXpwIjoiOE9WaHB2dGFJOW41UVZFUUszWDV5ZnNtQ2JyckxYZkUifQ.Ws6Ed_10z77Xn70AwLQzSxXSkePc5TrNP4hd_xRQQlPLg8mP5Fr6wMp6iDI-DX6ZIhy0tlY9tNDrbB1fbGe61p7MbA2OrmXKhhGoG3oaVW_dECfxLhhFvWr1WE1xnQW_B_CvhZA-BnRyReYpgXj5dZgdZk8hx2Uz5Y6gSa-raj_rngci0uBKpJ0VNfAAK-9VFPokRJc602ERFHd0kz6g4JA6BPBdiBSrttFlPAtqzDNNAKVmALORqPhgbjzSJdXteXTJCBT1YR14dH1oYdDz0LvuwpkClaZ-dRDPlsWvZ29_gccJO6xU9i1iwt0gJFhD7WpDjGDd3laO_HjXixmShw'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "s = requests.session()\n",
    "s.headers.update(headers)\n",
    "\n",
    "page = s.get(url)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "if page.status_code == 200:\n",
    "    print(\"Ok\", page)\n",
    "else:\n",
    "    print(f\"Hmm {page.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def scrape_article_ids(api_url, max_pages, headers):\n",
    "    job_data = []\n",
    "\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        page_url = f'{api_url}&page={page_number}'\n",
    "        \n",
    "        # Send an HTTP request to the API endpoint\n",
    "        session = requests.session()\n",
    "        session.headers.update(headers)\n",
    "        response = s.get(api_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "\n",
    "            # Extract advertiser IDs from each item in the 'data' list\n",
    "            for item in data['data']:\n",
    "                job = {\n",
    "                    \"job_id\": item['id'],\n",
    "                    \"title\": item['title'],\n",
    "                    \"company\": item['advertiser'][('description')]\n",
    "                }\n",
    "                job_data.append(job)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from the API. Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return job_data\n",
    "\n",
    "\n",
    "max_pages = 3\n",
    "\n",
    "# api url taken from Network -> Header\n",
    "api_url = 'https://id.jobstreet.com/api/jobsearch/v5/me/search?siteKey=ID-Main&sourcesystem=houston&userqueryid=279354b3755c5f7b91ccc1391bee4d41-2168443&userid=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&usersessionid=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&eventCaptureSessionId=5feb09fd-a69a-4c23-a2a4-fd73cab8c851&page=1&keywords=Information+Systems&pageSize=32&include=seodata,relatedsearches,joracrosslink,gptTargeting,pills&baseKeywords=Information+Systems&locale=id-ID&seekerId=581997302&solId=c64d160c-5b72-476a-bc37-3cf7a83a85fc&relatedSearchesCount=12'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.98',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlBQcjNlT0dQeFAwa2FRcnFxandmTyJ9.eyJodHRwOi8vc2Vlay9jbGFpbXMvY291bnRyeSI6IklEIiwiaHR0cDovL3NlZWsvY2xhaW1zL2JyYW5kIjoiam9ic3RyZWV0IiwiaHR0cDovL3NlZWsvY2xhaW1zL2V4cGVyaWVuY2UiOiJjYW5kaWRhdGUiLCJodHRwOi8vc2Vlay9jbGFpbXMvdXNlcl9pZCI6IjU4MTk5NzMwMiIsImlzcyI6Imh0dHBzOi8vbG9naW4uc2Vlay5jb20vIiwic3ViIjoiYXV0aDB8NjcxZWQ1MDVlODFiZjkwMDc0MDc5NDVlIiwiYXVkIjpbImh0dHBzOi8vc2Vlay9hcGkvY2FuZGlkYXRlIiwiaHR0cHM6Ly9zZWVrYW56Lm9ubGluZWF1dGgucHJvZC5vdXRmcmEueHl6L3VzZXJpbmZvIl0sImlhdCI6MTc0MjgwMjE1OSwiZXhwIjoxNzQyODA1NzU5LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIG9mZmxpbmVfYWNjZXNzIiwiYXpwIjoiOE9WaHB2dGFJOW41UVZFUUszWDV5ZnNtQ2JyckxYZkUifQ.AvVXPCCtxnJKLbstIhqfm4j6kux6WTqJsWH-7wC7ZuVfD67gNOJ2g11nZ-JmYlXoOErXMyBFob69MAdo6mwA4O0A08ozwjqrJkOLH_ZVKu1InvGC3b6wI1jIx55yfgD3DTac54d7f2Ejq79lI-7w8jD0FecVQgPyQtm6sN36j-Hn-vw-e4y7Y1tuNd4ZTOw7mwHNXiZScAoiRLAC9_-wPz2oHSJQa38oCxIo6mlh26skXg-lHbT3FupIcjOgPBMBjQGiBjfMAstyiDs2VqxPHlSjrzTFh_uOUuWw8pKWXO_DOBwRnx3eiID8JksLDRmgzGKXvucEn-wt-53OmwPd6w'\n",
    "}\n",
    "\n",
    "# job_id, titles, companies, locations, job_types, salarys = scrape_article_ids(api_url, max_pages, headers)\n",
    "scrape_article_ids(api_url, max_pages, headers)\n",
    "\n",
    "data = scrape_article_ids(api_url, max_pages, headers)\n",
    "\n",
    "with open('try-filtered.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
